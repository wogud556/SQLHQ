## 6장 DML튜닝

## 6.1 기본 DML 튜닝
#### DML이란?
- Data Manipulation Language 데이터 조작어 이다. 
- 말 그대로 보통 사용하는 SQL의 CRUD가 데이터 조작어에 포함이 된다. 
- CRUD는 CREATE(insert문), READ(select문), UPDATE(update문), DELETE(delete문)

### 6.1.1 DML성능에 영향을 미치는 요소
- DML성능에 영향을 미치는 요소는 8가지가 있다.
- ‘인덱스’, ‘무결성 제약’, ‘조건절’, ‘서브쿼리’, ‘REDO 로깅’, ‘UNDO 로깅’, ‘Lock’, ‘커밋’이 있다.

#### 6.1.1.1 인덱스와 DML성능
- 테이블에 레코드를 입력하려면 인덱스도 입력해야한다. 
- 테이블은 Freelist라는 것을 통해 입력할 수 있음.
- 블록을 할당받으나, 인덱스는 정렬된 자료구조이기 떄문에, 수직적 탐색을 통해 입력할 블록을 찾아야 한다, 따라서 인덱스에 입력하는 과정이 더 복잡하기 떄문에 DML성능에 미치는 영향도 크다.
- DELETE할때도 마찬가지로 레코드 하나 삭제시, 인덱스 레코드도 찾아서 삭제를 해야한다.
- UPDATE할 때는 변경된 컬럼을 참조하는 인덱스만 찾아서 변경해주면 된다.
- 단 테이블에서 한건 변경할 때마낟 인덱스에서는 두 개의 오퍼레이션이 발생하는데 인덱스가 정렬된 자료구조이기 때문이다.
- 인덱스 개수가 작으면 작을수록 핵심 트랜잭션 상에서 TPS가 그만큼 향상된다.
- 사진 하나 추가
- SOURCE테이블에 데이터를 100만개 넣어놓고 TARGET테이블에 SOURCE테이블의 데이터를 읽어서 데이터를 삽입을 하는 과정이다. 
- 시간은 4.95초가 걸렸다. 이번엔 인덱스 2개를 추가 한 후에 데이터를 삽입하면, 아까보다 약 8배나 느려진 38.98초가 걸린다. 
- 인덱스의 영향도가 이정도로 크다.

#### 6.1.1.2 무결성 제약과 DML성능
- 논리적으로 의미있는 자료만 저장되게 하는 데이터 무결성 규칙으로는 개체 무결성, 참조 무결성, 도메인 무결성, 사용자 정의 무결성이 있다.
- 각 무결성을 간단히 요약하면 개체 무결성은 기본키는 NULL값 허용불가, 값 중복 허용불가, 참조무결성은 외래키 값은 NULL이거나 참조릴레이션의 기본키 값과 동일, 도메인 무결성은 특정 속성의 값이 그 속성이 정의된 도메인에 속한 값이어야 함, 사용자 정의 무결성은 특정 값은 사용자가 정의한 제약조건에 만족.
- 해당 규칙을 어플리케이션으로도 구현할 수 있으나, DBMS에서 PK, FK, Check, Not null 같은 제약으로 완벽하게 데이터 무결성을 지킬 수 있다.
- 이들 중 PK, FK는 실제 데이터를 조회해야 하기 때문에 DML성능에 더 큰 영향을 미친다.
- 사진 하나 추가
- Check, Not Null 등은, 정의한 제약 조건을 준수하는지만 확인하면 된다.
아래는 인덱스와 pk제약을 모두 제거한 상태에서 100만건을 입력하는데 걸리는 시간이다.
- 결과로 pk제약과 인덱스, 일반인덱스2개가 존재한 상태에서 데이터 입력 시 38.98초(이전 테스트와 동일시간), pk제약과 인덱스만 존재 시 4.95초 둘 다 존재하지 않을 시 1.32초가 걸린다. 
- 그만큼 pk제약도 DML성능에 영향을 미친다.

#### 6.1.1.3 조건절과 DML성능
- SELECT문과 실행계획이 다르지 않으므로, 인덱스 튜닝 원리를 그대로 적용할 수 있다.(2,3장)
- 사진 하나 추가

#### 6.1.1.4 서브쿼리와 DML성능
- 사진 두장 추가
- SELECT문과 실행계획이 다르지 않으므로 조인튜닝 원리를 그대로 적용할 수 있다.(4.4장)

#### 6.1.1.5 Redo 로깅과 DML 성능
- 오라클이 데이터파일과 컨트롤 파일에 가해지는 모든 변경사항을 REDO 로그에 기록하는 행동을 Redo 로깅이라고 한다.
- 트랜잭션 데이터가 어떤 이유로 유실이 되었을 때, 트랜잭션을 재현함으로써 유실 이전으로 복구하는데 사용
- DML 수행시마다 Redo 로그를 생성 해야하기 떄문에, Redo로깅은 DML 성능에 영향을 미친다. 
- INSERT 작업에 대해 Redo 로깅 생략 기능을 제공하는 이유도 그렇다.
  - Redo 로그의 사용 목적은 첫 번째로 물리데이터 Media Fail 발생 시 데이터베이스를 복구하기 위해 사용된다. 
  - 두 번째로 캐시 리커버리(인스턴스 리커버리)를 사용하기 위해 사용이 된다. 캐시 리커버리는 위에 설명한 내용에 해당한다. 
  - 세 번째로 fast commit을 위해 사용이 된다. 
- 변경된 메모리 버퍼블록을 디스크 상의 데이터 블록에 반영하는 작업은 랜덤 액세스 방식으로 이루어 지므로, 매우 느린 반면에, 로그는 append 방식으로 기록하므로 상대적으로 빠르다. 
- 따라서 트랜잭션에 의한 변경사항을 append 방식으로 빠르게 로그 파일에 기록하고, 변경된 메모리 버퍼블록과 데이터 파일블록간 동기화는 적절한 수단을 이용해 배치 방식으로 일괄적으로 수행 하는데 있다.

#### 6.1.1.6 Undo 로깅과 DML성능
- Undo란 동일한 의미로 rollback이라는 말이라고 할 수 있다.
- redo가 트랜잭션을 재현함으로 과거상태를 현재상태로 되돌린다고 할 수 있고, undo는 트랜잭션을 롤백함으로써 현재상태를 과거상태로 되돌린다고 이야기할 수 있다.
- Undo도 Redo와 마찬가지로, DML이 수행될 때마다 Undo를 생성해야 하므로 DML성능에 영향을 미친다.
  - Undo의 사용 목적은 첫 번째로 트랜잭션에 의한 변경사항을 최종 커밋하지 않고, 롤백하고자 할 때 Undo데이터가 사용
  - 둘째로 Instance Crash 발생 후 Redo를 사용해 roll Forward단계가 완료되면, 최종 커밋 되지 않은 변경사항까지 모두 복구가 되기 때문에 시스템이 셧다운 된 시점에 아직 커밋되지 않았던 트랜잭션들을 모두 롤백해야 하는 상황에서 Undo 데이터가 사용이 된다.
  - 세 번째로 읽기 일관성을 위해 사용이 되는데 Consistent모드로 데이터를 읽는 오라클에선 동시트랜잭션이 많을수록 I/O가 증가하면서 성능저하가 이어지기 때문이다.

#### 6.1.1.7 Lock과 DML성능
- Lock을 필요 이상으로 자주, 길게 사용하거나 레벨을 높이면 DML 성능은 느려진다. 
- 그렇다고 Lock을 너무 적게, 짧게 사용하거나 필요 레벨 이하로 낮추면 데이터 품질이 나빠진다.
- 성능과 데이터 품질이 모두 중요한데, 둘은 트레이드 오프 관계여서 어렵다. 
- 두 마리 토끼를 다 잡으려면 세심한 동시성 제어가 필요하다.

#### 6.1.1.8 커밋과 DML 성능
- 커밋은 DML과 별개로 실행하지만, DML을 끝내려면 커밋까지 완료해야 하므로 서로 밀접한 관련이 있다. 
- 특히 DML이 Lock에 의해 블로킹 된 경우, 커밋은 DML 성능과 직결된다. 
- DML을 완료할 수 있게 LOCK을 푸는 열쇠가 커밋이기 때문
- 모든 DBMS가 Fast Commit을 구현하고 있는데, 구현 방식은 다르지만, 갱신한 데이터가 아무리 많아도 커밋 만큼은 빠르게 처리한다는 점은 동일.
- Fast Commit의 도움으로 커밋을 순간적으로 처리하긴 하지만 내부 메커니즘이 존재하여 절대 가벼운 작업은 아니다.


### 커밋의 내부 매커니즘

#### DB 버퍼캐시
- 일을 모았다가 한번에 처리하는 방식
- DB에 접속한 사용자를 대신하여 일을 처리하는 서버 프로세스는 버퍼캐시를 통해 데이터를 읽고 쓴다
- 변경된 블록을 모아 주기적으로 데이터 파일에 일괄 기록하는 작업은 DBWR(Database Writter) 프로세스가 맡는다.

#### Redo 로그버퍼
- Redo를 사용함으로 어떠한 이유로 데이터가 유실되었을 때 Redo로그를 이용하여 복구할 수 있다.
- 버퍼캐시가 휘발성이므로 DBWR프로세스가 dirty 블록들을 데이터파일에 반영할 때 까지 불안한 상태라고 생각할 수 있지만 redo에 기록된 로그를 이용하면 쉽게 데이터 복구를 할 수 있다.
- redo도 로그파일이기 때문에 오라클 내에서 IO가 발생할 수 있지만, 오라클은 로그버퍼를 사용하기 때문에 로깅 성능 문제가 해결이 가능하다.
- 사진 하나 추가 예정

#### 트랜잭션 데이터 저장 과정
1. DML문을 실행하면 Redo 로그 버퍼에 변경사항을 기록
2. 버퍼블록에 데이터를 변경한다. 물론 버퍼캐시에서 블록을 찾지 못하면 데이터파일에서 읽는 작업부터 수행
3. 커밋
4. LGWR프로세스가 Redo 로그버퍼 내용을 로그파일에 일괄 저장
5. DBWR 프로세스가 변경된 버퍼블록들은 데이터파일에 일괄 저장
   - 오라클은 데이터를 변경하기 전 로그부터 기록 이때를 Write Ahead Logging이라고 부른다.
   - Redo로그도 휘발성이지만, DBWR, LGWR가 주기적으로 Dirty블록과, Redo 로그버퍼파일을 기록하고, LGWR의 경우는 서버 프로세스가 커밋을 발행했다고 신호를 보낼때도 깨어나서 활동을 수행한다.
   - 이를 Log Force at Commit이라고 한다.
   - 서버프로세스가 변경한 버퍼블록들을 디스크에 기록하지 않았더라도 커밋시점에서  redo로그를 디스크에 안전하게 기록했다면 그 순간부터 트랜잭션의 영속성은 보장됨



#### 커밋 = 저장 버튼
- 커밋이란 저장버튼과 같다. 
- 여태까지 진행했던 내용들을 문서를 저장하듯이 저장을 하는 디스크에 기록하는 명령어라고 보면 된다.
- 저장을 완료할때까지 다음 작업을 진행할 수 없다.
- LGWR 프로세스에 신호를 보낸 후 작업을 완료했다는 신호를 받아야 다음 작업을 진행할 수 있다.
- LGWR 프로세스가 Redo 로그를 기록하는 작업은 디스크 I/O작업이다. 커밋이 그래서 느리다.
- 따라서 커밋을 안하는것도 문제지만 자주하는것도 문제가 되기 떄문에 논리적으로 잘 정의함으로 불필요한 커밋이 발행하지 않도록 해야한다.

### 6.1.2 데이터베이스 Call과 성능
#### 데이터베이스 Call
- 데이터베이스는 3단계로 Call을 실행한다.
  1. Parse Call
     - SQL 파싱과 최적화를 수행하는 단계. SQL과 실행계획을 라이브러리 캐시에서 찾으면 최적화 단계는 생략할 수 있다.
  2. Excute Call
     - 말 그대로 SQL을 실행하는 단계이다. DML은 이 단계에서 모든 과정이 끝나지만, SELECT 문은 FETCH단계를 거친다.
  3. Fetch Call
     - 데이터를 읽어서 사용자에게 결과 집합을 전송하는 과정으로, SELECT문에서만 나타난다. 
     - 전송할 데이터가 많을 때는 Fetch Call이 여러 번 발생
- 어디서 발생하냐에 따라 User Call, Recursive Call로도 나눌 수 있다.
- 사진하나추가
  1. User Call
     - 네트워크를 경유해 DBMS 외부로부터 인입되는 Call 이다. 
     - 그림상으로는 맨 왼쪽 클라이언트 단에 위치하나, DBMS입장에서 사용자는 WAS다. 3-Tier 아키텍쳐에서 User Call은 WAS서버에서 발생하는 Call이다.
  2. Recursive Call
     - DBMS 내부에서 발생하는 Call. 
     - SQL파싱과 최적화 과정에서 발생하는 데이터 딕셔너리 조회, PL/SQL로 작성한 사용자 정의 함수/프로시저/트리거에 내장된 SQL을 실행할 때 발생하는 Call이 여기에 해당 된다.
     - User Call이든, Recursice Call 이든, SQL을 실행할 때마다 Parse, Excute, Fetch Call단계를 거친다.
- 데이터베이스 Call 이 많으면 성능은 느릴 수밖에 없다. 특히 네트워크를 경유하는 User Call이 성능에 미치는 영향은 매우 크다.

#### 절차적 루프 처리
- 사진두장추가예정
- SOURCE 테이블에 레코드 100만개가 입력돼 있다. 
- PL/SQL 프로그램에서 SOURCE 테이블을 읽어 100만번 루프를 돌면서 건건이 TARGET 테이블에 입력을 하는 프로그램이다.
- 루프를 돌면서 건건이 Call이 발생하였지만, 네트워크를 경유하지 않는 Recursive Call이므로 그나마 29초 만에 수행을 마쳤다.
- 사진하나추가예정
- Java프로그램을 컴파일하고 실행한 결과이다. 
- Java 프로그램으로 수행하면 네트워크를 경유하는 User Call이므로 성능이 급격히 나빠진다.
29초 걸리던 PL/SQL 프로그램이 218초로 시간이 급격히 증가했다.

#### One SQL의 중요성
- 사진하나 추가예정
- Insert Into Select 구분을 수행한다.
- 사진에서와 같이 한번의 Call이 이루어졌기 떄문에 1.46초만에 끝이 났다.
- Java 프로그램과 비교하면 150배 빨라졌으며, one SQL의 중요성이 여기에 있다.
- 가급적 업무로직이 복잡해서 절차적으로 처리하지 않는 이상 One SQL로 처리하는 것이 좋다.
- Insert Into Select, 수정 가능한 조인 뷰, Merge 뷰 등이 절차적으로 구현된 One SQL을 구현하는데 아주 유용하다.

### 6.1.3 Array Processing 활용
- 절차적 프로그램을 실무에서 One SQL로 구현하는 일은 복잡한 업무 로직을 포함하는 경우가 많기 때문에 쉽지 않다.
- Array Processing이라는 기능을 활용하면 Call 부하를 One SQL을 사용하지 않고도 줄일 수 있다.
- 사진 두장 추가 예정
- 앞서 테스트한 절차적 프로그램을 PL/SQL에서 Array Processing으로 처리했더니 29.31초 수행하던거를 3.99초만에 수행이 완료되었다.
- JAVA 프로그램으로 프로그래밍 했을 때 절차적으로 218초 걸리던 것이 11.8초만에 수행을 완료하였다.
- 1만번에 한번 INSERT 하게끔 수정하여 100만번 CALL을 100번으로 줄여 나타난 성능 향상이다.
- 이 기능을 활용하여 One SQL에 준하는 성능 효과를 얻을 수 있다.

### 6.1.4 인덱스 및 제약 해제를 통한 대량 DML 튜닝
- 앞서 얘기했듯 무결성 제약 조건은 DML 성능에 큰 영향을 끼친다. 
- 그렇다고 OLTP나 트랜잭션 처리 시스템에서 이들 기능을 해제할 순 없다.
- 반면에 동시 트랜잭션 없이 대량데이터를 적재하는 배치프로그램에서 이들 기능을 해제함으로 큰 개선효과를 얻을 수 있다.
- 사진 하나 추가 예정
- 테스트를 위해 테이블 생성 후 SOURCE 테이블에 데이터 1000만건을 입력하였다.
- PK제약을 생성하면 Unique인덱스가 자동으로 생성된다. 
- 추가로 일반 인덱스를 하나 더 만들었으므로 인덱스는 총 두 개다. 이상태에서 TARGET 테이블에 1000만건을 입력을 해보았다.
- 사진 하나 추가예정
- PK제약과 인덱스가 있는 상태에서 1분 19초 걸렸다.

#### 6.1.4.1 PK제약과 인덱스 해제1-PK제약에 Unique 인덱스를 사용한 경우
- PK제약과 인덱스를 해제한 상태에서 데이터를 입력
- 사진하나 추가예정
- PK제약을 비활성화하면서 인덱스 DROP도 했다.
- 사진 하나 추가예정
- 일반 인덱스는 unusable상태, 이상태에서 데이터를 입력하려면 skip_unusable_indexes 파라미터를 아래와 같이 true로 설정해야 한다.
- 기본 값이 true이므로 이전에 변경한 적이 없다면 여기서 굳이 설정을 변경하지 않아도 된다.
- 무결성 제약과 인덱스를 해제함으로 빠르게 insert할 준비가  됐다. 
- 다시 1000만건 입력 후, 경과를 확인해보았는데, 5.84초 만에 수행을 마쳤다. 그 후 pk제약을 활성화 하고, 일반 인덱스를 재생성하면 작업 완료
pk제약을 활성화 하면 pk인덱스는 자동으로 생성이 된다.

- 데이터 입력시간과 제약 활성화 및 인덱스 재생성 시간을 합쳐도 기존보다 훨씬 더 빨리 작업을 마쳤다. 
- 인덱스 및 무결셩 제약이 DML성능에 미치는 영향이 이렇게 크다.
- PK제약을 활성화 하면서 NOVALIDATE 옵션을 사용한 것도 시간을 단축하는데 한몫을 하였고, 이는 기입력된 데이터에 대한 무결성 체크를 생략하도록 하는 옵션
데이터 무결성에 확신이 없다면 데이터를 입력하기 전에 아래 쿼리를 확인해야한다.


#### 6.1.4.2 PK제약과 인덱스 해제 2 - PK 제약에 NONUnique 인덱스를 사용한 경우
- 이전 테스트는 X1인덱스는 Unusable 상태로 변경했지만, pk인덱스는 제약을 비활성화하면서 아예 drop해버렸다
- pk인덱스를 unusable한 상태에서 데이터를 입력할 수 없기 떄문이다. 아래 실행결과를 확인
- 사진하나 추가
- pk 인덱스를 drop하지 않고 unusable한 상태로 데이터를 입력하고싶다면, 아래와 같이 pk제약에 non-unique인덱스를 사용하면 됨.
- 사진 하나 추가
- 아래와 같이 pk제약을 비활성화하고, 인덱스를 unusable상태로 변경, pk제약을 비활성화 했지만, 인덱스는 drop하지 않고 남겨짐
- 사진 두장 추가
- 대량 insert작업을 하는데 아무 문제가 없어졌다.
- 사진 하나 추가
- 데이터 입력 시간과 제약 활성화 및 인덱스 재생성 시간을 합쳐도 기존보다 훨씬 더 빨리 작업을 마침

### 6.1.5 수정 가능 조인 뷰

#### 6.1.5.1 전통적인 방식의 UPDATE
- 종종 보이는 UPDATE문
- 사진 두개 추가 
- 아래와 같이 고칠 수 있다
- 사진 하나 추가
- 수정 내역은 교체 대상 컬럼을 한번에 수행하는 것
- 위의 방식은 한달 이내의 고객별 거래 데이터를 두 번 조회하기 떄문에 총 고객 수와 한달 이내 거래 고객 수에 따라 성능이 좌우된다.
- 총 고객 수가 많다면 EXIST 서브쿼리를 아래와 같이 해시 세미 조인으로 유도하는 것을 고려할 수 있다.
- 사진 하나 추가
- 특정 고객들의 거래량이 많아 update 발생량이 많으면, 아래와 같이 변경하는 것을 고려할 수 있다. 하지만 락이 걸리는 것은 물론 이전과 같은 값으로 갱신되는 비중이 높기 때문에 redo로그 발생량이 증가해 오히려 비효율적일 수 있다.
- 다른 테이블과 조인이 필요할 때 전통적인 update문을 사용하면 비효율을 완전히 해소할 수 있음

#### 6.1.5.2 수정 가능 조인 뷰
- 수정 가능 조인 뷰를 활용하면 참조 테이블과 두 번 조인하는 비효율을 없앨 수 있다.
(12c 이상 버전에서만 정상적으로 실행된다. 11g버전에서는 실행 안됨)
- 조인 뷰란 FROM절에 두 개 이상 테이블을 가진 뷰를 가리키며, 수정가능 조인 뷰는 말 그대로 입력 수정 삭제가 허용되는 조인 뷰를 말한다.
- 사진 두장 추가
- 위 예시는 조인뷰로 job = 'CLERK'인 레코드의 loc를 모두 ‘SEOUL’로 변경하는 것을 허용한다면 어떻게 되는지 보여주는 쿼리이다. 
- 결과로보면 job = 'CLERK'인 사원이 10 20 30 부서에 속해있는데  UPDATE수행시 loc가 모두 seoul로 다 바뀌었을 것이다. 
- 다만 다른 job을 가진 사원의 부서 소재지까지 바뀌는건 원하는 결과가 아닐 것
- 사진 3개 추가
- 문제가 없어보이는 update사진이다 하지만 옵티마이저가 어느 테이블이 1쪽 집합인지 알 수 없기 때문에 발상하는 에러이다. 
- 이때는 insert나 delete도 되지 않는다.
- 사진 하나 추가
- 위와 같이 pk제약을 설정하거나, unique인덱스를 생성해야 수정가능 조인을 통한 입력/수정/삭제가 가능하다.
- 위와 같이 pk제약을 설정하면, emp테이블은 키-보존 테이블이 되고, dept 테이블은 비 키-보존 테이블로 남는다


#### 6.1.5.3 키 보존 테이블이란?
- 조인된 결과 집합을 통해서도 중복 값 없이 UNIQUE 하게 식별이 가능한 테이블을 말한다.
- UNIQUE한 1쪽 집합과는 조인되는 테이블이어야 조인된 결과집합을 통한 식별이 가능하다.
- 사진 하나 추가
- 사진은 앞에서 수행했던 테스트의 EMP_DEFT_VIEW에서 rowid를 출력한 결과인데, dept_id에 중복 값이 나타나고 있다.
- emp_rid에는 중복 값이 없으며, rowid와 일치한다. 단적으로 키 보존 테이블이란 뷰에 rowid를 제공하는 테이블이다
- dept테이블에 unique테이블을 없애면, 키 보존 테이블이 없기 때문에 뷰에서 rowid를 출력할 수 없다.

#### 6.1.5.4 ORA-01779 오류 회피
- 키 보존 테이블이 아닌 곳에 업데이트 할려는 경우 보통 1:M관계가 있는 조인의 경우 M이 키 보존 테이블인데,  1쪽에 업데이트 하려면 발생하는 오류 
- 사진 두장 추가
- DEPT테이블에 AVG_SAL 컬럼을 추가하고, EMP로부터 부서별 평균 급여를 계산해서 방금추가한 컬럼에 반영한 UPDATE문이다.


- 11이하버전에서는 UPDATE문을 실행시, 아래와 같이 ORA-01779에러가 발생한다. EMP테이블은 DEPTNO로 GROUP BY 했으므로 DEPTNO컬럼으로 조인한 DEPT테이블은 키가 보존되는데도 옵티마이저가 불필요한 제약을 가한 것
- 이럴땐 bypass_ujvc를 사용해서 제약을 회피할 수 있다. 그러나 11g부터 힌트가 사용 불가하고 위 update문은 실행할 수 없다. 뒤에 나올 Merge문으로 바꿔줘야 한다.

- 이러한 기능은 12c부터 기능이 개선이 되었고, 예를들어 위 사진은 고객_t 고객번호에 Unique 인덱스가 없으면 아래 쿼리는 어떤 버전에서도 실행이 안된다.

- 위 사진은 12c에서 고객_t 테이블을 group by함으로ㅆ ORA-01779 에러를 회피한 것
- 배치프로그램이나 데이터 이행 프로그램에서 사용하는 중간 임시 테이블에는 일일이 PK제약이나 인덱스를 생성하지 않으므로 이 패턴이 유용할 수 있다.

6.1.6 MERGE문 활용
Merge 문이란 : 조건과 비교해서 테이블에 조건에 맞는 데이터가 없으면 insert, 있으면 update문을 수행한다
ex)
MERGE INTO 조작할 테이블명
USEING (데이터)
DELETE WHERE 조건

WHEN MATCHED THEN
       UPDATE SET 컬럼 = 값, 컬럼 = 값
       DELETE WHERE 조건

WHEN NOT MATCHED THEN
INSERT (컬럼....) VALUES (값.....)
WHERE 조건;
- DW에서 가장 흔히 발생하는 오퍼레이션은 기간계 시스템에서 가져온 신규 트랜잭션 데이터를 반영함으로써 두 시스템간 데이터를 동기화하는 작업이다.
- 예를들어 고객 테이블에 발생하는 변경분 데이터를 DW에 반영하는 프로세스는 
1. 전일 발생한 변경테이터를 기간계 시스템으로부터 추출 -> 2. CUSTMOER_DELTA 테이블을 DW시스템으로 천송 -> 3. DW 시스템으로 적재 등이 있다.
- 이때 3번 데이터 적재 작업을 효과적으로 지원하기 위해 오라클 9i부터 MERGE문이 도입되었다.
- MERGE문은 TARGET 테이블과 LEFT OUTER방식으로 조인해서 조인에 성공하면 UPDATE, 실패하면 INSERT한다.
- MERGE문을 UPSERT(UPDATE + INSERT) 리고도 부르는 이유이다.

1. Optional Clauses
- 아래와 같이 UPDATE와 INSERT를 선택적으로 처리할 수 있다.

- 이 확장기능을 통해 아래와 같이 수정가능 조인 뷰 기능을 대체할 수 있게 되었다.


2. Conditional Operations
- On 절에 기술한 조인문 외에 아래와 같이 추가로 조건절이 기술이 가능하다.


3. DELETE Clause
- 이미 저장된 데이터를 조건에 따라 지우는 기능도 제공한다.

- 예시된 merge문에서 update가 이루어진 결과로서 탈퇴 일시가 null아닌 레코드만삭제한다는 사실이다. 즉 탈퇴일시가 null 이 아니었어도 merge문을 수행한 결과가 null이면 삭제하지 않는다.
- merge문은 delete절은 조인에 성공한 데이터만 삭제할 수 있으며, Source테이블에서 삭제된 데이터는 terget테이블에서도 지우고 싶을텐데, merge문 delete절이 그 역할까지는 못한다는 뜻이다.
- 조인엔 실패한 데이터는 update할 수도 없고, delete할 수도 없다는 사실을 아래 그림을 통해 확인





4. Merge into 활용 예

-저장하려는 레코드가 기존에 있는 것이라면 update, 그렇지 않으면 insert하려고 하는데, 그 때 위 쿼리와 같이 처리하면 sql항상 두 번씩 수행한다.(좌) 오른쪽의 경우는 최대 두 번 실행한다(1번 수행할 수도 있다는 뜻)

- 위와 같이 Merge문을 수행하면 sql을 한번만 수행한다.



* 수정가능 조인 뷰, MERGE문 VS MERGE문

 -실행 계획만 같다면 UPDATE문을 사용하든 MERGE문을 사용하든 상관은 없다
그러나 아래와 같은 패턴 때문에 상관할 일이 자꾸 생긴다고 한다.

이 쿼리를 사용하는 이유는 UPDATE 대상 쿼리 개수를 파악할 수 있따. 즉 SELECT문을 먼저 만들어서 데이터검증을 마친 후 바깥에 MERGE문을 씌우는 방식으로 SQL을 개발하는 것. MERGE문 ON 절에는 ROWID를 사용한다.
이 쿼리가 성능이 안좋은 이유는 UPDATE 대상 테이블인 EMP를 두 번 엑세스 하기 때문이다.
차라리 아래 UPDATE문을 사용하면서 SELECT로 데이터 검증을 마친 후 UPDATE를 하는 것이 더 좋다는 생각.



